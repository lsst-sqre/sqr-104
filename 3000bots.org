#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_PLUGINS: (highlight)
#+OPTIONS: toc:nil num:nil
#+REVEAL_HLEVEL: 1
#+REVEAL_THEME: white
#+REVEAL_EXTRA_CSS: ./css/local.css
#+REVEAL_INIT_OPTIONS: slideNumber: "c/t"
#+REVEAL_PLUGINS: (highlight)
#+LATEX_COMPILER: lualatex
#+LATEX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setsansfont{Verdana}
#+LATEX_HEADER: \setmainfont{Verdana}
#+AUTHOR: Adam Thornton
#+EMAIL: athornton@lsst.org
#+TITLE: How 500 Real Users Are Worse Than 3000 Bot Users

* How 500 Real Users Are Worse Than 3000 Bot Users

[[./assets/logo.png]]

* NSF-DOE Vera C. Rubin Observatory

I work for NSF-DOE Vera C. Rubin Observatory (henceforth, "Rubin Observatory" or just "Rubin").

It's an optical-band telescope with an 8.4m mirror, located on Cerro Pachon in Chile.
We will begin survey operations by the end of the year.

During the ten-year survey we will collect 15-20TB per night of astronomical data.
The primary catalog will be about 15PB, and total data holdings about 500PB.

* Rubin Science Platform

Much of my work is on the Rubin Science Platform (henceforth, "RSP").  It provides services to the Rubin science community:

- Jupyter notebooks with a kernel containing our data analysis pipelines.
- A search-and-visualization tool for Rubin astronomical data.
- API access to Rubin astronomical data.
- Et al.

It is all deployed through [[https://phalanx.lsst.io][Phalanx]], a GitOps ArgoCD-and-therefore-Helm-based Kubernetes deployment framework.

In particular, I often focus on the RSP Notebook Aspect.

** RSP Notebook Aspect

The Notebook Aspect is designed to:

- Offer a JupyterLab Notebook service to scientists,
- With the Rubin Data Management analysis pipeline available,
- To allow access to a small but arbitrary slice through the data,
- In order to facilitate quick investigation of hypotheses.

** Data Preview 1

On June 30, 2025, we had our first public release of observational catalog data.

The primary site for astronomical community access is [[https://data.lsst.cloud][our Internet Data Facility hosted by Google]].

There are other Rubin Science Platform sites; this talk is only concerned with our Google Cloud Platform-hosted production instance and its integration counterpart where we did the actual scaletesting.

** Our JupyterHub/JupyterLab environment

- Based on [[https://github.com/jupyterhub/zero-to-jupyterhub-k8s][Zero to JupyterHub]].
- Individual user namespaces (and individual user domains).
- Prepulled, very large, Lab container images.
- [[https://github.com/lsst-sqre/nublado/tree/main/controller][Custom spawner]].
  - Split session management and Kubernetes actions.
  - Because of individual namespaces, the K8s actor must be highly privileged.
- [[https://github.com/lsst-sqre/rsp-jupyter-extensions][Custom Lab extensions]].
- Deployed with [[https://github.com/lsst-sqre/phalanx][Phalanx]].

** Usage expectations

On the RSP, what you get is a four-core, 16GB Lab pod.

This is intended to be a (fairly anemic) laptop in the cloud, for lightweight hypothesis investigation, not a supercomputer.

For things that need significant amounts of memory or CPU, the user is
expected to use a batch system instead.

** Site setup

We run on Google Kubernetes Engine, with autoscaling enabled.

Each node has 32 cores and 128GB of memory (n2-standard-32).

On a typical Wednesday morning (17 Sep 2025), we had 123 users on 5 Lab nodes.

[[./assets/nodes.png]]

* Scale Testing

On the order of ten thousand people (the US/Chilean population of astronomers) have data rights and therefore could use the system if they wanted to.

Many of those ten thousand don't do astronomy that Rubin Observatory will facilitate (e.g. radio astronomers).

We somewhat arbitrarily took three thousand as an upper bound, as "more people than will likely try out DP1".

** Population assumptions

We expected our Data Preview 1 audience to be those people really excited about Rubin, who wanted to understand our processing pipelines in advance of taking real survey data.

Thus we expected them to be somewhat more sophisticated users than our eventual mid-operations median user would be.

** Actual numbers

A few days before the June 30 DP1 date, we had 998 non-bot users on the production GKE-hosted RSP.

On July 8, we had 1165 non-bot users; on August 18, we had 1295; September 17, 1401.
15% of our users signed up in the last week before DP1.

Our account approval process is our rate-limiting step: manual approval of each account, to ensure that it's someone with legitimate data rights, is cumbersome.

Our highest observed concurrency thus far has been about 550 non-bot users.

* Testing methodology

We have a service we created, called =mobu=, that is able to run various payloads (mostly Jupyter notebooks) within the RSP.

It is mostly used for automated regression testing as the analysis pipelines have evolved.

By design, it is indistinguishable (from JupyterHub's point of view) from an astronomer logging in and doing work.
It uses the Hub API to establish a JupyterLab session and then can run Python code within JupyterLab kernels, either as entire notebooks or as individual statements.

** Overall strategy: get to 3000

Our strategy was to get to 3000 simultaneous users, which we did not expect to succeed immediately.
We intended to iterate over successive performance and functionality bottlenecks until we hit that concurrency goal.

We began in late January 2025, and finished our JupyterHub/Lab testing in late April, doing one three-hour scaletesting session a week.

*** Initial Concurrency Results

Our very first test was 1000 users who logged in, did not do anything (not even start a pod), and logged out; success.

3000 users failed because of our own lack of foresight.
At some time previously, we'd capped Mobu at 1000 concurrent tasks, because surely that would be plenty.

We raised that limit and got 36000 K8s events per minute, as expected, and moved on to spawning user pods.
Hub user tracking is fast and not a bottleneck.

*** Spawning pods

100 simultaneous users "running" a codeless notebook (no Python execution, just text) worked fine, and GKE autoscaling was performing as advertised.

1000 users failed: at 300 users we started to get spawn timeouts as the K8s control plane was failing to keep up.

Because the control plane was unreliable, user pod deletion was also sometimes timing out and failing.

*** Remediation

Scaletesting in February and March was devoted to chasing down timeouts and internal Hub and controller errors.

We found some race conditions in our controller code that we would have been difficult to find in a reasonably-loaded system.

We realized that our practice of cloning tutorial repositories into user labs at startup was hitting GitHub rate limit problems at scale, and modified our tutorial strategy accordingly.

More memory and CPU for mobu and the Hub helped, but we still were getting timeouts from Lab-to-Hub communications.

*** The JupyterHub database

Eventually we realized that JupyterHub uses a single database connection, and all database operations are [[https://jupyterhub.readthedocs.io/en/stable/explanation/database.html][synchronous and block the rest of the process]].

The only remediation we could immediately take therefore was drastically reducing the frequency of lab activity reports and culler polling.
This cut the number of database requests (mostly writes) sharply and helped significantly.

For instance: we only cull idle users after about a week, so an hourly activity poll (rather than the default five minutes) was sufficient granularity.

*** Desired JupyterHub enhancements

The single-threading on the database is becoming problematic.

We eagerly await the ability to scale the Hub horizontally.

One database session per request and multiple Hub instances, as discussed in the link above, will be fantastic for us.

*** Other things we found

[[https://github.com/IBM/jupyter-tools/blob/87296dd13ab43b905c7657d17e3eac7371e90fc1/docs/configuration.md][IBM's jupyter-tools]] has some very useful tuning advice.
This is where, for instance, we got our recommendations for culling and activity polling.

Google imposes a 200-requests-per-second limit on the K8s control plane.
We ended up smearing out our pod startups by dispatching them in batches rather than all at once in a single tight loop.

Ghcr.io imposes a high but finite rate limit for pulling container images.
We worked around this by hosting the both the init and Lab containers in Google Artifact Registry.

[[./assets/k8scp-200.png]]

*** Early April: meeting testing criteria

After we'd made the above changes we got 3000 simultaneous start-then-execute-a-print-statement-then-quit Labs.

At this point, with the DP1 deadline approaching, we moved on to other services.

* Data Preview 1 Reality

We got 500-ish simultaneous users when Data Preview 1 went live.
That was within our expectations, and maybe even a little disappointing (even if it's still 1/40th of all the professional astronomers in the world).

This went less smoothly than we had hoped: spawn failures started to occur at a far lower user count than we had achieved in scaletesting.

The problem was in the proxy, not the Hub or the controller.
It wasn't the memory exhaustion we'd already seen and fixed.

** How Are 500 Real Users Worse Than 3000 Bot Users?

The very simple answer: *bots log out*.

** Configurable Hub Proxy and Websockets

Abandoned open websockets wreck CHP v4.

Human users, despite the fact that we give them a perfectly good menu item to save their work and shut down their pod, don't use it.
/At best/ they close their browser tab, and most of them don't even do that.

CHP v5 addresses this problem adequately.
After adopting v5, that concurrency problem vanished and we haven't seen it again.

At the moment we are coping well with 500-ish simultaneous users doing science work.

** Post-DP1 lessons

We are also validating assumptions about data access.
This involves notebooks that make large queries that require a lot of memory.

We found we needed to make our overcommital ratio much more tunable.
A normal real-user workload allows a high overcommital ratio (we've found 4 to work well).

If your workload is 50 bot users all simultaneously doing very memory-intensive work, when the Labs all ask for their whole memory limit at once (even though each process stays just under its limit), node memory runs out.

Most of our remaining bottlenecks are neither in Hub nor Lab but in the services notebooks consume.

** Your Platform Probably Isn't Just A Notebook Service

At the very least, you probably have some sort of A&A sytem, a Notebook service, and a data source.
You may have services that sit in between your notebooks and your data store.
We certainly do.

If so, you will likely need to (internally) rate limit access to other services, especially if they perform significant computation on the user's behalf.

We have [[https://gafaelfawr.lsst.io][Gafaelfawr]] for this (thus it's built into the A&A system).
You're going to want to use something similar.

* Problematic Usage

- Cryptominers: Google warned us based on their outbound connection patterns.
  That was good because we might not have noticed otherwise, because...
- Naïve users will indeed just hammer the system.
  One user looped over a huge result set, asking for a "postage stamp" image for every single object.
  That was two cutouts a second, and would have taken months to complete.
- You absolutely need disk quotas if you provide per-user persistent storage.
  Before we imposed quotas, one user used more disk space than all thousand others combined.

* Summary of Scaling Lessons

- Use CHPv5.
- Onboarding and offboarding are important.
  Think them through before the users arrive /en masse/.
- Have some kind of internal resource-limiting service so your notebook users can't crush your other services.
- Can you differentiate deliberate abuse from clueless enthusiasm?

Sometimes you have to downgrade a few users' experience to keep the overall experience tolerable for everyone.

* Links

- [[https://github.com/lsst-sqre/nublado][Nublado]] [[https://nublado.lsst.io][(docs)]]
- [[https://github.com/lsst-sqre/phalanx][Phalanx]] [[https://phalanx.lsst.io][(docs)]]
- [[https://github.com/lsst-sqre/gafaelfawr][Gafaelfawr]] [[https://gafaelfawr.lsst.io][(docs)]]
- [[https://github.com/lsst-sqre/sqr-104/blob/main/3000bots.org][This talk]] [[./3000bots.pdf][(pdf)]]
