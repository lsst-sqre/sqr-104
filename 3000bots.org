#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_PLUGINS: (highlight)
#+OPTIONS: toc:nil num:nil
#+REVEAL_HLEVEL: 1
#+REVEAL_THEME: white
#+REVEAL_EXTRA_CSS: ./css/local.css
#+REVEAL_INIT_OPTIONS: slideNumber: "c/t"
#+REVEAL_PLUGINS: (highlight)
#+LATEX_COMPILER: lualatex
#+LATEX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setsansfont{Verdana}
#+LATEX_HEADER: \setmainfont{Verdana}
#+AUTHOR: Adam Thornton
#+EMAIL: athornton@lsst.org
#+TITLE: How 500 Real Users Are Worse Than 3000 Bot Users

* How 500 Real Users Are Worse Than 3000 Bot Users

[[./assets/logo.png]]

* NSF-DOE Vera C. Rubin Observatory

I work for NSF-DOE Vera C. Rubin Observatory (henceforth, "Rubin Observatory" or just "Rubin").

It's an optical-band telescope with an 8.4m mirror, located on Cerro Pachon in Chile.
We will begin survey operations by the end of the year.

During the ten-year survey we will collect 15-20TB per night of astronomical data.
The primary catalog will be about 15PB, and total data holdings about 500PB.

* Rubin Science Platform

Much of my work is on the Rubin Science Platform (henceforth, "RSP").  It provides services to the Rubin science community:

- Jupyter notebooks with a kernel containing our data analysis pipelines.
- A search-and-visualization tool for Rubin astronomical data.
- API access to Rubin astronomical data.
- Various other services.

My work often revolves around the Jupyter notebook service, and scaling that is what this talk is about.

** Our JupyterHub/JupyterLab environment

- Based on [[https://github.com/jupyterhub/zero-to-jupyterhub-k8s][Zero to JupyterHub]].
- Individual user namespaces (and individual user domains).
- Prepulled, very large, Lab container images.
- [[https://github.com/lsst-sqre/nublado/tree/main/controller][Custom spawner]].
  - Split session management and Kubernetes actions.
  - Because of individual namespaces, the K8s actor must be highly privileged.
- [[https://github.com/lsst-sqre/rsp-jupyter-extensions][Custom Lab extensions]].
- Deployed with [[https://github.com/lsst-sqre/phalanx][Phalanx]].

** Data Preview 1

On June 30, 2025, we had our first public release of observational catalog data.

The primary site for astronomical community access is [[https://data.lsst.cloud][our Internet Data Facility hosted by Google]].

There are other Rubin Science Platform sites; this talk is only concerned with our Google Cloud Platform-hosted production instance and its integration counterpart where we did the actual scaletesting.

** Usage expectations

On the RSP at Google, what you get is a four-core, 16GB Lab pod.

This is intended to be the equivalent of a (fairly anemic) laptop in the cloud, for lightweight hypothesis investigation, not a supercomputer.

For things that need significant amounts of memory or CPU, the user is expected to use a batch system instead.

** Site setup

We run on Google Kubernetes Engine, with autoscaling enabled.

Each node has 32 cores and 128GB of memory (n2-standard-32).

On a typical Wednesday morning (17 Sep 2025), we had 123 users on 5 Lab nodes.

[[./assets/nodes.png]]

* Scale Testing

There are about twenty thousand astronomers worldwide.
Half of those are from US or Chilean institutions and therefore can get an account on the Google RSP if they want one.
(It's more complicated than this but that's in the ballpark.)

Many of those ten thousand don't do astronomy that Rubin Observatory will facilitate (e.g. radio astronomers).

We somewhat arbitrarily took three thousand as an upper bound, as "more people than will likely try out DP1".

** Population assumptions

We expected our Data Preview 1 audience to be those people really excited about Rubin, who wanted to understand our processing pipelines in advance of taking real survey data.

Thus we expected them to be somewhat more sophisticated users than our eventual mid-operations median user would be.

** Actual numbers

A few days before the June 30 DP1 date, we had 998 non-bot users on the production GKE-hosted RSP.
This was measured as "number of home directories" and therefore only counts users who logged in to the system at least once.

On July 8, we had 1165 non-bot users; on August 18, we had 1295; September 17, 1401.
15% of our users signed up in the last week before DP1.

Our account approval process was (and is) our rate-limiting step: manual approval of each account, to ensure that it's someone with legitimate data rights, is cumbersome.

Our highest observed concurrency thus far has been about 550 non-bot users.

* Testing methodology

We have a service we created, called =mobu=, that is able to run various payloads (mostly Jupyter notebooks) within the RSP.

It is mostly used for automated regression testing and for exercising new features as the analysis pipelines have evolved.

By design, a =mobu=-driven bot user is indistinguishable (from JupyterHub's point of view) from an astronomer logging in and doing work.
Mobu uses the Hub API to establish a JupyterLab session and then can run Python code within JupyterLab kernels, either as entire notebooks or as individual statements.

** Overall goal: get to 3000

Our victory condition was to get to 3000 simultaneous users each running a trivial Python workload.
We did not expect to succeed immediately.

We began in late January 2025, and finished our JupyterHub/Lab testing in late April, doing one three-hour scaletesting session a week.

*** Initial Concurrency Results

Our very first test was 1000 users who logged in, did not do anything (not even start a pod), and logged out; success.

3000 users only failed because of our own lack of foresight: we'd designed =mobu= with the assumption that 1000 concurrent tasks would be more than enough.
Hub user lifecycle management is nowhere near a bottleneck.

Then we actually started spawning Lab pods.

100 simultaneous users "running" a codeless notebook (no Python execution, just text) worked fine, and GKE autoscaling was performing as advertised.

1000 users failed: at 300 users we started to get spawn timeouts as the K8s control plane failed to keep up with the requests.

*** Remediation

Scaletesting in February and March was devoted to chasing down timeouts and internal Hub and controller errors.

- We found race conditions in our controller code that would have been difficult to find in a reasonably-loaded system.
- We had to use a less aggressive polling cadence to reconcile the controller's view of the world with reality.
- We realized that our practice of cloning tutorial repositories into user labs at startup was hitting GitHub rate limit problems at scale.

More memory and CPU for mobu and the Hub helped, but we still were getting timeouts from Lab-to-Hub communications.

*** The JupyterHub database

Eventually we realized that JupyterHub uses a single database connection, and all database operations are [[https://jupyterhub.readthedocs.io/en/stable/explanation/database.html][synchronous and block the rest of the process]].

The only remediation we could immediately take was to drastically reduce the frequency of lab activity reports for culler polling.

This made it possible to get to our goal without significant reduction in functionality.
Polling each user for activity every five minutes is gratuitous if our culling threshold is on the order of a week.

*** Desired JupyterHub enhancements

The single-threading on the database is becoming problematic.
We can reduce poll frequency to a certain degree but that doesn't scale indefinitely.

[[https://jupyterhub.readthedocs.io/en/stable/explanation/database.html][As the Hub database page explains]], work is underway to move to a database-session-per-request model.

This will allow scaling the Hub horizontally, and we intend to be early and enthusiastic adopters when that becomes possible.

*** Other things we found

[[https://github.com/IBM/jupyter-tools/blob/87296dd13ab43b905c7657d17e3eac7371e90fc1/docs/configuration.md][IBM's jupyter-tools]] has some very useful tuning advice specifically for stress-testing Hub.
This is where, for instance, we got our recommendations for culling and activity polling.

GKE imposes a 200-requests-per-second limit on the K8s control plane.
We worked around this by dispatching pod startups in batches rather than all at once.
However, this ultimately constrains the scale of a single cluster at GKE.

Ghcr.io imposes a high but finite rate limit for pulling container images.
We worked around this by hosting the both the init and Lab containers in Google Artifact Registry, which did not exhibit this behavior.

[[./assets/k8scp-200.png]]

*** Early April: meeting testing criteria

After we'd made the above changes we got 3000 simultaneous start-then-execute-a-print-statement-then-quit Labs.

At this point, with the DP1 deadline approaching, we declared victory and moved on to other services.

* Data Preview 1 Reality

We got 500-ish simultaneous users when Data Preview 1 went live.
That was within our expectations, and maybe even a little disappointing (even if it's still more than two percent of all the professional astronomers in the world).

This went less smoothly than we had hoped: spawn failures started to occur at a far lower user count than we had achieved in scaletesting.

The problem was in the proxy, not the Hub or the controller.
It wasn't the memory exhaustion we'd already seen and fixed.

** How Are 500 Real Users Worse Than 3000 Bot Users?

The very simple answer: *bots log out*.

** Configurable Hub Proxy and Websockets

Abandoned open websockets wreck CHP v4.

Human users, despite the fact that we give them a perfectly good menu item to save their work and shut down their pod, don't use it.
/At best/ they close their browser tab, and most of them don't even do that.

CHP v5 addresses this problem adequately.
After adopting v5, that concurrency problem vanished and we haven't seen it again.

At the moment we are coping well with 500-ish simultaneous users doing science work.

** Post-DP1 lessons

We are also validating assumptions about data access.
This involves notebooks that make large queries that require a lot of memory.

We found we needed to make our overcommital ratio more tunable.
A normal real-user workload allows a high overcommital ratio (we've found 4 to work well).

If your workload is 50 bot users all simultaneously doing very memory-intensive work, when the Labs all ask for their whole memory limit at once (even though each process stays just under its limit), node memory runs out.

Most of our remaining bottlenecks are neither in Hub nor Lab but in the services notebooks consume.

** Your Platform Probably Isn't Just A Notebook Service

At the very least, you probably have some sort of A&A sytem, a Notebook service, and a data source.
You may have services that sit in between your notebooks and your data store.
We certainly do.

If so, you will likely need to (internally) rate limit access to other services, especially if they perform significant computation on the user's behalf.

We have [[https://gafaelfawr.lsst.io][Gafaelfawr]] for this (thus it's built into our A&A system).
You're going to want to use something similar.

* Problematic Usage

- Cryptominers: Google warned us based on their outbound connection patterns.
  With four cores and no GPU, I'm pretty sure they didn't make much money.
- We might not have noticed otherwise, because a user eating their entire CPU allocation for days on end isn't necessarily suspicious.
  (It is, however, indicative that they /should/ be using batch instead.)
  Na√Øve users will indeed just hammer the system.
  One user looped over a huge result set, asking for a "postage stamp" image for every single object.
  That was two cutouts a second, and would have taken months to complete.
- You absolutely need disk quotas if you provide per-user persistent storage.
  Before we imposed quotas, one user used more disk space than all the thousand others combined.

* Summary of Scaling Lessons

- Use CHPv5.
- Onboarding and offboarding are important.
  Think them through before the users arrive /en masse/ and be prepared to handle surges.
- Have some kind of internal resource-limiting service so your notebook users can't crush your other services.
- Can you differentiate deliberate abuse from clueless enthusiasm?  Do you care to?

Sometimes you have to downgrade a few users' experience to keep the overall experience tolerable for everyone.

* Links

- [[https://github.com/lsst-sqre/nublado][Nublado]] [[https://nublado.lsst.io][(docs)]]
- [[https://github.com/lsst-sqre/phalanx][Phalanx]] [[https://phalanx.lsst.io][(docs)]]
- [[https://github.com/lsst-sqre/gafaelfawr][Gafaelfawr]] [[https://gafaelfawr.lsst.io][(docs)]]
- [[https://ls.st/3000bots][This talk]] [[./3000bots.pdf][(pdf)]] [[https://github.com/lsst-sqre/sqr-104/blob/main/3000bots.org][(source)]]
